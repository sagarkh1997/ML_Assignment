{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6036d74",
   "metadata": {},
   "source": [
    "# 1. Newsgroup dataset\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. In this assignment we are going to use a **subset** of this dataset to learn a bit about unsupervised learning methods in machine learning. To load this dataset we are going to use scikit-learn library which you have worked with a couple of times through semester."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd546a52",
   "metadata": {},
   "source": [
    "# 2. Loading data and preprocessing\n",
    "\n",
    "To ensure that your results are reproducable make sure to set the seed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b28346a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17164\\AppData\\Local\\Programs\\Python\\Python310\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(os.path.dirname(sys.executable))\n",
    "\n",
    "SEED = 123\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e2f3d",
   "metadata": {},
   "source": [
    "## 2.1. downloading and loading data\n",
    "\n",
    "You can download both training set and test set separately using sklearn apis. [Here](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) you can find sample codes for loading dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f17edda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# YOUR CODE HERE\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers,footers','quotes'))\n",
    "df_221 = pd.DataFrame(newsgroups_train.target_names)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers,footers','quotes'),return_X_y=True)\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers,footers','quotes'),return_X_y=True)\n",
    "newsgroups_train_df = pd.DataFrame(newsgroups_train).T\n",
    "print(newsgroups_train_df.shape)\n",
    "newsgroups_train_df.columns = ['newstitle','target']\n",
    "newsgroups_test_df = pd.DataFrame(newsgroups_test).T\n",
    "newsgroups_test_df.columns = ['newstitle','target']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b6e79",
   "metadata": {},
   "source": [
    "## 2.2 filtering target classes\n",
    "\n",
    "Explore dataset and get familiar with it. Then, implement `filter_split_x_y` function which extracts `selected_targets` classes from data and splits the data into two lists of X and ys. Please don't change `selected_targets` list is this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e7c855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_targets = [1, 7, 10, 13, 15, 16, 17]\n",
    "\n",
    "def filter_split_X_y(raw_data, selected_targets):\n",
    "    \"\"\"\n",
    "    This function inputs a newsgroup dataset and filters it based on your selected labels\n",
    "    then returns two lists of datapoints and labels separately\n",
    "    \n",
    "    \"\"\"\n",
    "    result_data = raw_data[raw_data['target'].isin(selected_targets)]\n",
    "    return result_data['newstitle'],result_data['target']\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "train_x, train_y = filter_split_X_y(newsgroups_train_df, selected_targets)\n",
    "test_x, test_y = filter_split_X_y(newsgroups_test_df, selected_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1043e6",
   "metadata": {},
   "source": [
    "**2.2.1.** Print the name of classes in your training set along with `selected_targets` you can use `target_names` attribute of `newsgroups_train`. Make sure you include this output in your PDF report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97bd0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_221_ans = df_221.iloc[selected_targets]\n",
    "df_221_ans.columns=['target_names']\n",
    "df_221_ans.index.name = 'target'\n",
    "#df_221_ans.to_csv('df_221.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc7fc3d",
   "metadata": {},
   "source": [
    "## 2.3. vectorizing documents\n",
    "\n",
    "There are several ways that one can use to vectorize a whole document. In this programming assignment we are going to use **TF-IDF** method. Please go ahead and watch [this short video](https://www.youtube.com/watch?v=D2V1okCEsiE) to understand how tf-idf works and then answer the following questions.\n",
    "\n",
    "**2.3.1.** What does TF-IDF stand for?<br>\n",
    "**2.3.2.** Why don't we only use term frequency of the words in a document as its feature vector? what is the benefit of adding inverse document frequency?<br>\n",
    "**2.3.3.** Calculate the tf-idf vectors of the following two documents, assuming this is the entire corpus:\n",
    "![Documents](tfidf.png)\n",
    "\n",
    "\n",
    "Now we are going to use scikit-learn to calculate the tf-idf vectors of each document in our twenty newgroups dataset. Write a function that accepts both `train_x` and `test_x` and returns the tf-idf vectors of them in numpy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb89355b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize_dataset(train_x, test_x):\n",
    "    \"\"\"\n",
    "    returns vectorized numpy array of training and test set\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_vec = vectorizer.fit_transform(train_x)\n",
    "    test_vec = vectorizer.transform(test_x)\n",
    "    return train_vec.toarray(),test_vec.toarray()\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "train_vec, test_vec = vectorize_dataset(train_x, test_x)\n",
    "print(type(train_vec),type(test_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae221f4",
   "metadata": {},
   "source": [
    "# 3. dimension reduction\n",
    "\n",
    "In the previous section we built features vectors for each of the documents. However, these feature vectors are highly sparse and are not easy to cluster or visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e8656",
   "metadata": {},
   "source": [
    "## 3.1. Sparsity\n",
    "\n",
    "Answer these questions to understand the sparsity of these vectors:\n",
    "\n",
    "**3.1.1** Count the number of non-zeros in each row of the `train_vec` matrix.<br>\n",
    "**3.1.2** What is the average number non zero elements in each row? <br>\n",
    "**3.1.3** On average what percentage of elements in each row have non-zero elements?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd68658c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4081, 56153)\n",
      "170.56187209017398\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_vec_df = pd.DataFrame(train_vec)\n",
    "print(train_vec_df.shape)\n",
    "# test_vec_df = pd.DataFrame(test_vec)\n",
    "count_non_zero = (train_vec_df!=0).astype(int).sum(axis=1)\n",
    "\n",
    "#3.1.1 count_non_zero\n",
    "count_non_zero_list = count_non_zero.tolist()\n",
    "# print(count_non_zero_list)\n",
    "#3.1.2 count_non_zero_avg\n",
    "count_non_zero_avg = sum(count_non_zero_list)/len(count_non_zero_list)\n",
    "print(count_non_zero_avg)\n",
    "# count_non_zero_list_avg = count_non_zero_list.\n",
    "count_non_zero_perc = (train_vec_df!=0).astype(int).mean(axis=1)\n",
    "count_non_zero_perc = count_non_zero_perc*100\n",
    "#3.1.3 average percentage of elements\n",
    "count_non_zero_perc_list = count_non_zero_perc.tolist()\n",
    "# print(count_non_zero_list[0]/52057)\n",
    "# print(count_non_zero_perc_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0ca7a",
   "metadata": {},
   "source": [
    "## 3.2. SVD\n",
    "\n",
    "Use `TruncatedSVD` module in scikit-learn to perform SVD on the dataset. Reduce your dimensions to 3 and perfomr SVD for 100 iterations (**note: the number of iterations parameter is a special component for an alternative way to solve SVD that we did not study in class. You do not need to know what it means**). Make sure to use the random seed given at the beginning of the notebook.\n",
    "\n",
    "**Note: you should only fit your SVD/UMAP models on train data**\n",
    "\n",
    "**3.2.1.** What portion of the variance in your dataset is explained by each of the SVD dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dae89b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01618638 0.00617073 0.00540306]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def fit_and_transform_svd(train_vec, test_vec):\n",
    "    \"\"\"\n",
    "    trains a svd model and generate reduced dimension vectros from training and test dataset\n",
    "    returns: 2 numpy arrays of size <number of documents>*3\n",
    "    \"\"\"\n",
    "    # YOUR CODE\n",
    "    train_vec_csr = csr_matrix(train_vec)\n",
    "    test_vec_csr = csr_matrix(test_vec)\n",
    "    svd = TruncatedSVD(n_components=3, n_iter=100, random_state=SEED)\n",
    "    train_svd = svd.fit_transform(train_vec_csr)\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    test_svd = svd.transform(test_vec_csr)\n",
    "    return train_svd,test_svd\n",
    "    \n",
    "\n",
    "\n",
    "# print(type(train_vec),type(test_vec))    \n",
    "train_svd, test_svd = fit_and_transform_svd(train_vec, test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb47d96",
   "metadata": {},
   "source": [
    "## 3.3 UMAP\n",
    "\n",
    "UMAP is another dimensionality reduction method that works in a more complex way. [This video](https://www.youtube.com/watch?v=6BPl81wGGP8) explains on a high level how it performs dimension reduction.\n",
    "Apply UMAP and get the embeddings for both training and test datasets. Use the previously given `random_state` and reduce dimensions to 3. Also use **cosine similarity** as the similarity metric of UMAP:\n",
    "\n",
    "- n_components=3\n",
    "- metric='cosine'\n",
    "- random_state=SEED\n",
    "- low_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91225027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0133362  0.1758109  5.5556164]\n",
      " [ 1.8398644 -1.2699696  4.3199425]\n",
      " [ 1.2948723  1.8366706  6.8801923]\n",
      " ...\n",
      " [ 4.175102   2.1275587  5.723664 ]\n",
      " [ 4.3053517 -1.7793598  7.8886843]\n",
      " [ 0.8120112 -1.355712   4.2428293]]\n",
      "0         7\n",
      "3         1\n",
      "5        16\n",
      "6        13\n",
      "16        1\n",
      "         ..\n",
      "11299    17\n",
      "11300    15\n",
      "11305    17\n",
      "11309    13\n",
      "11312     1\n",
      "Name: target, Length: 4081, dtype: object\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "import umap.umap_ as umap\n",
    "\n",
    "def fit_and_transform_umap(train_vec, test_vec):\n",
    "    # YOUR CODE\n",
    "    train_vec_csr = csr_matrix(train_vec)\n",
    "    test_vec_csr = csr_matrix(test_vec)\n",
    "    reducer = umap.UMAP(n_components=3,metric='cosine',random_state=SEED,low_memory=True)\n",
    "    train_umap = reducer.fit_transform(train_vec_csr)\n",
    "    test_umap = reducer.transform(test_vec_csr)\n",
    "    return train_umap,test_umap\n",
    "        \n",
    "    \n",
    "    \n",
    "train_umap, test_umap = fit_and_transform_umap(train_vec, test_vec)\n",
    "print(train_umap)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec2f314",
   "metadata": {},
   "source": [
    "## 3.4. visualization\n",
    "\n",
    "install `babyplots` library which provides some nice 3D visaulizations. Then, visualize both of embeddings (SVD and UMAP) of the **training data** using the given function. **make sure you put a screenshot of the visualizations inside your write up**. You can use it to explain your answers to the questions.\n",
    "\n",
    "<span style=\"color:red\"> Please don't use babyplot inside <b>jupyter lab</b> as it is still not fully supported and produces some errors </span>\n",
    "\n",
    "Hint: you need to run the function in a separate cell to get the visualization\n",
    "\n",
    "**3.4.1.** Based on your observation, what is the difference between SVD and UMAP embeddings? 1-2 sentences should suffice.<br>\n",
    "**3.4.2.** Which one do you prefer to use for a classification task? why? 1-2 sentences should suffice <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4cb27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from babyplots import Babyplot\n",
    "\n",
    "def visualize(datapoints, labels):\n",
    "    bp = Babyplot(background_color=\"#ffffffff\", turntable=True)\n",
    "    bp.add_plot(\n",
    "        datapoints,\n",
    "        \"pointCloud\",\n",
    "        \"categories\",\n",
    "        np.array(labels).tolist(),\n",
    "        {\n",
    "            \"colorScale\": \"Dark2\", \n",
    "            \"showLegend\": True\n",
    "        }\n",
    "    )\n",
    "    return bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99728cf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\17164\\Desktop\\Python\\ML_Assignment\\assignment4\\PA-4.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/17164/Desktop/Python/ML_Assignment/assignment4/PA-4.ipynb#ch0000020?line=0'>1</a>\u001b[0m train_umap_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(train_umap)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/17164/Desktop/Python/ML_Assignment/assignment4/PA-4.ipynb#ch0000020?line=1'>2</a>\u001b[0m train_umap_df[[\u001b[39m'\u001b[39m\u001b[39mumap1\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mumap2\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mumap3\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(train_umap_df[:,\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/17164/Desktop/Python/ML_Assignment/assignment4/PA-4.ipynb#ch0000020?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(train_umap_df\u001b[39m.\u001b[39mcolumns)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/frame.py?line=3502'>3503</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/frame.py?line=3503'>3504</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/frame.py?line=3504'>3505</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/frame.py?line=3505'>3506</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/frame.py?line=3506'>3507</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\range.py:388\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/indexes/range.py?line=385'>386</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/indexes/range.py?line=386'>387</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/indexes/range.py?line=387'>388</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[0;32m    <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/indexes/range.py?line=388'>389</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m    <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/indexes/range.py?line=389'>390</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_loc(key, method\u001b[39m=\u001b[39mmethod, tolerance\u001b[39m=\u001b[39mtolerance)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:5637\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/indexes/base.py?line=5632'>5633</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_indexing_error\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m   <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/indexes/base.py?line=5633'>5634</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/indexes/base.py?line=5634'>5635</a>\u001b[0m         \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/indexes/base.py?line=5635'>5636</a>\u001b[0m         \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/17164/AppData/Roaming/Python/Python310/site-packages/pandas/core/indexes/base.py?line=5636'>5637</a>\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "train_umap_df = pd.DataFrame(train_umap)\n",
    "train_umap_df[['umap1','umap2','umap3']] = pd.DataFrame(train_umap_df[:,0].tolist())\n",
    "print(train_umap_df.columns)\n",
    "#print(train_umap_df[:,0])\n",
    "#visualize(train_umap_df[:,0], train_y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d668518",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(train_svd, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad357739",
   "metadata": {},
   "source": [
    "# 4. Kmeans clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b626b9ea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this part we are going to perform KMeans clustering on the generated embeddings of our training dataset. One major challenge with KMeans is to find the optimum number of clusters to use for clustering. Here, based on our training data, we know that there are 7 clusters in the dataset. However, we want to find the number of optimum clusters solely based on our data. In this part we are going to use [`average silhouette coefficient`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) to evaluate our clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ec389",
   "metadata": {},
   "source": [
    "## 4.1. clustering and evaluation\n",
    "\n",
    "**4.1.1** What is the range of possible values of silhouette coefficients? <br>\n",
    "**4.1.2** Describe what a silhouette score of -1 and 1 mean?<br>\n",
    "**4.1.3.** Use `silhouette score` and `KMeans` from sklearn library to find the optimum number of clusters in your `train_umap`. Don't forget to use `SEED` as your kmeans `random_seed`. In order to do this try different values of cluster numbers from 5 to 20. Choose the one that results in the best score. <br>\n",
    "**4.1.4.** Plot silhouette score for different values of `n_clusters` (a plot with `n_clusters` on the x-axis and silhouette score on the y-axis) and find the best value for `n_cluster`. Don't forget to put the plot in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7e753f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30499595, 0.31430882, 0.3090369, 0.3272844, 0.33932102, 0.3318959, 0.33244315, 0.3481158, 0.35379916, 0.34355864, 0.34665114, 0.34920922, 0.36026663, 0.36132315, 0.3472128, 0.35398644]\n",
      "CPU times: total: 52.1 s\n",
      "Wall time: 9.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_clustering_scores(train_vectors,n_for_clusters) -> KMeans:\n",
    "    \"\"\"\n",
    "    This function calculates KMeans model for different values of n_cluster and calculates silhouette score\n",
    "    for each of them. Then it returns a list of silhouette scores\n",
    "    RETURNS: \n",
    "        list of scores\n",
    "    \"\"\"\n",
    "    # YOUR CODE\n",
    "    clustering_scores = []\n",
    "    for n in n_for_clusters:\n",
    "        km = KMeans(n_clusters=n,random_state=SEED)\n",
    "        km.fit_predict(train_vectors)\n",
    "        clustering_scores.append(silhouette_score(train_vectors,km.labels_,metric='euclidean'))\n",
    "    \n",
    "    return clustering_scores\n",
    "    \n",
    "\n",
    "n_for_clusters = np.arange(5,21)\n",
    "clustering_scores = get_clustering_scores(train_umap_df,n_for_clusters)\n",
    "print(clustering_scores)\n",
    "## plot scores\n",
    "# YOUR CODE\n",
    "\n",
    "\n",
    "## Find best number of clusters\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077c798",
   "metadata": {},
   "source": [
    "## 4.2. Making a kmeans classifier\n",
    "\n",
    "Now that we have the best number of clusters, run KMeans again with the best number of clusters associated with the best sihouette score (on `train_umap`). \n",
    "\n",
    "After running KMeans you get some number of clusters but each of them have some cluster label that does not necessarily match the trainig labels. Try to implement `get_cluster_mapping` function that inputs your KMeans model and the training labels and returns a dictionary that maps each of the cluster labels to one of the training labels. For example your cluster 0 might correspond to training label of 17.\n",
    "\n",
    "Hint: In your `get_cluster_mapping` function, find the training label of the majority of data points in each cluster. For example, if your cluster 0 contains 100 data points of label 12 and 5 data points of label 15, the majority of data points in cluster 0 come from label 12. Therefore, your output dictionary should contain the mapping `0: 12`\n",
    "\n",
    "**4.2.1** show your mapping (resulted dictionary) inside your project report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8186a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run KMeans again with best parameter you calculated in the previous section\n",
    "# YOUR CODE HERE\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "def get_cluster_mapping(clustering: KMeans, original_labels):\n",
    "    \"\"\"\n",
    "    input: a clustering and original labels of the data\n",
    "    returns: a dictionary that maps each cluster number to an original label\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "# Feed the generated kmeans clustering and your training labels to the following function\n",
    "cluster_mapping = get_cluster_mapping(clustering, train_y)\n",
    "print(cluster_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302677d",
   "metadata": {},
   "source": [
    "## 4.3. Analyzing clusters\n",
    "\n",
    "**4.3.1.** Are there any two clusters in your clustering output with the same training label (for example, are there two clusters which both have same training label)? Use your visualizations and describe why?<br>\n",
    "**4.3.2.** Write the function bellow that returns nearest samples to a cluster center. Use this function and explain why there are overlaps in your labels? <br>\n",
    "**4.3.3.** Can you infere the overlapping label(s) by checking out most central samples? check with original labels.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def most_central_samples(clustering: KMeans, cluster_id, k=3):\n",
    "    \"\"\"\n",
    "    returns the text of k most central samples in the specified cluster_id\n",
    "    \"\"\"\n",
    "    # YOUR CODE\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ac552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4443bf7",
   "metadata": {},
   "source": [
    "## 4.4. evaluate your kmeans model on test dataset\n",
    "\n",
    "**4.4.1.** Using the generated mapping, and your clustering model, predict the labels of test dataset (you can use the embeddings of test data that you generated by umap `test_umap`) <br>\n",
    "**4.4.2.** Calculate the accuracy of model <br>\n",
    "**4.4.3.** Calculate both micro and macro values of precision, recall and F1 score <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958ebc1",
   "metadata": {},
   "source": [
    "# **574 Only** 5.1 KNN classification\n",
    "\n",
    "Using sklearn `KneighborsClassifier`, classify news data. Then, evaluate your model using the test set.\n",
    "\n",
    "**5.1.1.** Train two seperate KNN models on both SVD and UMAP embeddings. Use `n_neighbors=100`. <br>\n",
    "**5.1.2.** Evaluate your model on test datas (`test_umap` and `test_svd`). Which model performs better? Why? <br>\n",
    "**5.1.3.** Calculate macro and micro precision recall and fscore for `test_umap`. Which one of the two do you prefer for evaluating your model? why? <br>\n",
    "**5.1.4.** Shortly describe why the two sets of values (macro and micro) are so similar in this case. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbf5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9051d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282b13d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
